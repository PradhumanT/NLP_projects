# -*- coding: utf-8 -*-
"""anlp-a1 (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rW3v9bS9r2_70SwEku6EBAjT4jGsdjec
"""

import re
from collections import Counter
import numpy as np
import torch
from torch.utils.data import Dataset
import nltk
from tqdm import tqdm
import random
from collections import defaultdict
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import math

nltk.download('punkt')

from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Device available now:', device)

#Tokenizing the text into a list of sentences

paragraph = str()
sentences = list()

with open('/kaggle/input/auguste/Auguste_Maquet.txt', 'r') as file:
    for line in file:
        if (len(line.strip()) > 0):
            paragraph += line.strip() + ' '
        else:
            if (paragraph!=""):
                sentences += sent_tokenize(paragraph)
                paragraph = ""

# Tokenize the last paragraph if it's not empty
if (paragraph!=""):
    sentences += sent_tokenize(paragraph)
print(len(sentences))

#Spliting the above obtained sentences into training set, validating set and test set

valid_size = 10000
test_size = 20000

random.shuffle(sentences)

valid_list = sentences[:valid_size]
test_list = sentences[valid_size:valid_size+test_size]
train_list = sentences[valid_size+test_size:]

#loading pre-trained GloVE embeddings
fpath = '/kaggle/input/sarcasm-with-wiki-and-golve-embeddings/glove.6B.300d.txt'

#finding the embedding for 'unk' token

a = []

with open(fpath, 'r') as f:
    for line in f:
        l_toks = line.strip().split()
        if(l_toks[0]=='unk'):
            a = l_toks[1:]

e = list(map(float, a))
unk_emb = torch.tensor(e)

#Obtaining a dictionary for all other embeddings

embeddings = defaultdict(lambda: unk_emb)
with open(fpath, 'r') as f:
    for line in f:
        l_toks = line.strip().split()
        embeddings[l_toks[0]] = torch.tensor(list(map(float, l_toks[1:])))

# We are basically just creating embeddings for the input that we are supposed to give to the model and are also obtaining labels for these respective input
class Data(Dataset):
    def __getitem__(self, index):
        return (self.labels[index], self.five_gram_embeds[index])

    def __len__(self):
        return len(self.labels)

    def __init__(self, file, embeddings, vocab=None, w2idx=None):
        self.file = file
        self.embeddings = embeddings
        self.labels = list()
        if not vocab:
            self.vocab = list()
        else:
            self.vocab = vocab
        self.freq_dictionary = defaultdict(lambda:0)
        self.five_gram_embeds = list()
        self.unk_token = '<unk>'
        if not w2idx:
            self.word2idx = dict()
        else:
            self.word2idx = w2idx
        self.embeddings_list = list()





        #Creating vocabulary during training

        if not vocab:
            for line in file:
                words = list()
                for word in word_tokenize(line):
                    word = word.lower()
                    words.append(word)
                self.vocab.extend(words)

            m_set = set(self.vocab)
            self.vocab = list(m_set)
            self.vocab.append(self.unk_token)

       #Obtaining embeddings for only those words, which are in the vocabulary and then replacing it

            for idx, word in enumerate(self.vocab):
                self.word2idx[word] = idx

            for word in self.vocab:
                self.embeddings_list.append(self.embeddings[word])

            self.embeddings = torch.stack(self.embeddings_list)




        for line in file:
            indices = list()
            embeds = list()

            for word in word_tokenize(line):
                if word.lower() in self.vocab:
                    indices.append(self.word2idx[word.lower()])
                    embeds.append(self.embeddings[self.word2idx[word.lower()]])
                else:
                    indices.append(self.word2idx[self.unk_token])
                    embeds.append(self.embeddings[self.word2idx[self.unk_token]])

            for i in range(len(indices) - 5):
                self.five_gram_embeds.append(torch.stack(embeds[i:i+5]))
                self.labels.append(indices[i+5])


        self.labels = torch.tensor(self.labels)
        self.five_gram_embeds = torch.stack(self.five_gram_embeds)

train_dataset = Data(file=train_list, embeddings=embeddings)
embeddings=train_dataset.embeddings
print(len(train_dataset.vocab))

valid_dataset = Data(file=valid_list, embeddings=embeddings, vocab=train_dataset.vocab, w2idx=train_dataset.word2idx)
test_dataset = Data(file=test_list, embeddings=embeddings,  vocab=train_dataset.vocab, w2idx=train_dataset.word2idx)

#Model

class NNLM(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.layer1 = nn.Sequential(
            nn.Linear(300 * 5, 300),
            nn.ReLU()
        )
        self.layer2 = nn.Sequential(
            nn.Linear(300, 300),
            nn.ReLU()
        )
        self.layer3 = nn.Linear(300, vocab_size)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


    def forward(self, batch):
        inputs = batch.view(batch.shape[0], -1)
        return torch.log_softmax(self.layer3(self.layer2(self.layer1(inputs))), dim=1)

#loss function

def loss_func(lm, dataset):
        loss = 0
        for batch in dataset:
            comb_embeds = batch[1]
            comb_embeds = comb_embeds.to(lm.device)
            outs = lm.forward(comb_embeds)
            labels = batch[0]
            labels = labels.to(lm.device)
            l = nn.CrossEntropyLoss()(outs, labels)
            loss=loss+l.item()
        return loss

#perplexity function

def perplexity_func(lm, dataset):
        perp = 0
        count = 0
        for batch in dataset:
            comb_embeds = batch[1]
            comb_embeds = comb_embeds.to(lm.device)
            outs = lm.forward(comb_embeds)
            labels = batch[0]
            labels = labels.to(lm.device)
            loss = nn.CrossEntropyLoss()(outs, labels)
            count = count + 1
            perp = perp + math.exp(loss.item())
        count = count*(len(batch))
        return (perp/count)

#Training the model

lm = NNLM(len(train_dataset.vocab)).to(device)
optim = torch.optim.SGD(lm.parameters(), lr=0.01)


train_data = DataLoader(train_dataset, batch_size=256, shuffle=True)
valid_data = DataLoader(valid_dataset, batch_size=256, shuffle=True)

for i in range(8):
    for batch in train_data:
        optim.zero_grad()
        comb_embeds = batch[1]
        comb_embeds = comb_embeds.to(lm.device)
        outs = lm.forward(comb_embeds)
        labels = batch[0]
        labels = labels.to(lm.device)
        loss = nn.CrossEntropyLoss()(outs, labels)
        loss.backward()
        optim.step()

    val_perp = perplexity_func(lm, valid_data)
    val_loss = loss_func(lm, valid_data)
    print(f"Epoch {i+1} | Val Loss: {val_loss} | Val Perplexity: {val_perp}")

torch.save(lm, 'nnlm.pth')

#Testing our model

test_data = DataLoader(test_dataset, batch_size=256, shuffle=True)
model = torch.load('nnlm.pth')
model = model.to(device)


#generate perplexity files

with open('2020115016-LM1-test-perplexity.txt', 'w') as f:
    for line in test_list:

            indices = list()
            embeds = list()
            five_gram_embeds = list()
            labels = list()

            for word in word_tokenize(line):
                if word.lower() in train_dataset.vocab:
                    indices.append(train_dataset.word2idx[word.lower()])
                    embeds.append(train_dataset.embeddings[train_dataset.word2idx[word.lower()]])
                else:
                    indices.append(train_dataset.word2idx['<unk>'])
                    embeds.append(train_dataset.embeddings[train_dataset.word2idx['<unk>']])


            for i in range(len(indices) - 5):
                five_gram_embeds.append(torch.stack(embeds[i:i+5]))
                labels.append(indices[i+5])


            if(len(five_gram_embeds)==0):
                continue
            five_gram_embeds = torch.stack(five_gram_embeds)
            five_gram_embeds = five_gram_embeds.to(device)
            outs = model(five_gram_embeds)
            x = outs.shape[-1]
            outs = outs.view(-1, x)
            labels = torch.tensor(labels)
            labels = labels.to(device)
            labels = labels.view(-1)
            loss = nn.CrossEntropyLoss()(outs, labels).item()
            prp = math.exp(loss)/labels.size(0)
            sen = line[:-1] + '\t' + str(prp) + '\n'
            f.write(sen)

with open('2020115016-LM1-train-perplexity.txt', 'w') as f:
    for line in train_list:
        
            indices = list()
            embeds = list()
            five_gram_embeds = list()
            labels = list()
           
            for word in word_tokenize(line):
                if word.lower() in train_dataset.vocab:
                    indices.append(train_dataset.word2idx[word.lower()])
                    embeds.append(train_dataset.embeddings[train_dataset.word2idx[word.lower()]])
                else:
                    indices.append(train_dataset.word2idx['<unk>'])
                    embeds.append(train_dataset.embeddings[train_dataset.word2idx['<unk>']])
            
           
            for i in range(len(indices) - 5):
                five_gram_embeds.append(torch.stack(embeds[i:i+5]))
                labels.append(indices[i+5])
                
            
            if(len(five_gram_embeds)==0):
                continue
            five_gram_embeds = torch.stack(five_gram_embeds)
            five_gram_embeds = five_gram_embeds.to(device)
            outs = model(five_gram_embeds)
            x = outs.shape[-1]
            outs = outs.view(-1, x)
            labels = torch.tensor(labels)
            labels = labels.to(device)
            labels = labels.view(-1)
            loss = nn.CrossEntropyLoss()(outs, labels).item()
            prp = math.exp(loss)/labels.size(0)
            sen = line[:-1] + '\t' + str(prp) + '\n'
            f.write(sen)
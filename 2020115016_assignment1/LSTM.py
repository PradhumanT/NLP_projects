# -*- coding: utf-8 -*-
"""anlp-a2 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dBaM1HOv3TKl8ScDrOCRGnRBMkUndSJE
"""

from torch.utils.data import Dataset, DataLoader
import os

import torch.nn as nn
from tqdm import tqdm
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
import regex as re
from collections import defaultdict
import sys
import torch.nn.functional as F
from torch import tensor
import random
import torch
import math



device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Device available now:', device)

#Tokenizing the text into a list of sentences

paragraph = str()
sentences = list()

with open('/kaggle/input/auguste-maquet/Auguste_Maquet.txt', 'r') as file:
    for line in file:
        if (len(line.strip()) > 0):
            paragraph += line.strip() + ' '
        else:
            if (paragraph!=""):
                sentences += sent_tokenize(paragraph)
                paragraph = ""

# Tokenize the last paragraph if it's not empty
if (paragraph!=""):
    sentences += sent_tokenize(paragraph)
print(len(sentences))

#Spliting the above obtained sentences into training set, validating set and test set

valid_size = 10000
test_size = 20000

random.shuffle(sentences)

valid_list = sentences[:valid_size]
test_list = sentences[valid_size:valid_size+test_size]
train_list = sentences[valid_size+test_size:]

#loading pre-trained GloVE embeddings
fpath = '/kaggle/input/sarcasm-with-wiki-and-golve-embeddings/glove.6B.300d.txt'

#finding the embedding for 'unk' token

a = []

with open(fpath, 'r') as f:
    for line in f:
        l_toks = line.strip().split()
        if(l_toks[0]=='unk'):
            a = l_toks[1:]

e = list(map(float, a))
unk_emb = torch.tensor(e)

#Obtaining a dictionary for all other embeddings

embeddings = defaultdict(lambda: unk_emb)
with open(fpath, 'r') as f:
    for line in f:
        l_toks = line.strip().split()
        embeddings[l_toks[0]] = torch.tensor(list(map(float, l_toks[1:])))

class Data(Dataset):
    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        return (self.lstm_input[index], self.labels[index])


    def __init__(self, file, embeddings, vocab=None, w2idx=None):
            self.filepath = file
            self.embeddings = embeddings

            self.labels = list()
            self.max_len = -1
            self.lstm_input = list()
            self.freq_dictionary = defaultdict(lambda:0)
            if not vocab:
                self.vocab = list()
            else:
                self.vocab = vocab
            if not w2idx:
                self.word2idx = dict()
            else:
                self.word2idx = w2idx

            self.embeddings_list = list()


            if not vocab:
                for line in file:
                    words = list()
                    for word in word_tokenize(line):
                        word = word.lower()
                        words.append(word)
                    self.max_len = max(len(words), self.max_len)
                    self.vocab.extend(words)

                self.vocab = list(set(self.vocab))

                self.vocab.append('<unk>')
                self.vocab.insert(0, '<pad>')

                for idx, word in enumerate(self.vocab):
                    self.word2idx[word] = idx

                flag = 0
                for word in enumerate(self.vocab):
                    if(flag==0):
                        flag = 1
                        continue
                    self.embeddings_list.append(self.embeddings[word])
                self.embeddings_list.append(self.embeddings['<unk>'])
                self.embeddings = torch.stack(self.embeddings_list)
            else:
                for line in file:
                    self.max_len = max(len(word_tokenize(line)), self.max_len)

            for line in file:
#
                indices = list()
#
                for word in word_tokenize(line):
                    if word.lower() in self.vocab:
                        indices.append(self.word2idx[word.lower()])
                    else:
                        indices.append(self.word2idx['<unk>'])

                a = indices
                b = indices[1:]
                pad_len = self.max_len - len(indices)
                a += [0]*pad_len
                b += [0]*(pad_len+1)

                self.lstm_input.append(torch.tensor(a))
                self.labels.append(torch.tensor(b))

            self.lstm_input = torch.stack(self.lstm_input)
            self.labels = torch.stack(self.labels)

train_dataset = Data(file=train_list, embeddings=embeddings)
valid_dataset = Data(file=valid_list, embeddings=embeddings, vocab=train_dataset.vocab, w2idx=train_dataset.word2idx)
test_dataset = Data(file=test_list, embeddings=embeddings,  vocab=train_dataset.vocab, w2idx=train_dataset.word2idx)
tr_embeddings=train_dataset.embeddings
print(len(train_dataset.vocab))

#Model

class LM_LSTM(nn.Module):
    def __init__(self, vocab_size, embedding_matrix):
        super().__init__()
        self.embedding_layer = nn.Embedding(vocab_size, 300).from_pretrained(embedding_matrix)
        self.fc_layer = nn.Linear(300, vocab_size)
        self.lstm_layer = nn.LSTM(300, 300, num_layers=2, batch_first=True)


    def forward(self, batch):
        a = self.embedding_layer(batch)
        a,b = self.lstm_layer(a)
        a = self.fc_layer(a)
        return F.log_softmax(a, dim=2)

#loss function
def loss_func(lm, dataset):
    loss = 0

    for batch in dataset:
        lstm_input = batch[0]
        lstm_input = lstm_input.to(device)
        outs = lm.forward(lstm_input)
        x = outs.shape[-1]
        outs = outs.view(-1, x)
        labels = batch[1]
        labels = labels.to(device)
        labels = labels.view(-1)
        l = loss_fn = nn.CrossEntropyLoss(ignore_index=0)(outs, labels).item()
        loss += l
    return loss

def perplexity_func(lm, dataset):
    perp = 0
    count = 0

    for batch in dataset:
        lstm_input = batch[0]
        lstm_input = lstm_input.to(device)
        outs = lm.forward(lstm_input)
        x = outs.shape[-1]
        outs = outs.view(-1, x)
        labels = batch[1]
        labels = labels.to(device)
        labels = labels.view(-1)
        loss = loss_fn = nn.CrossEntropyLoss(ignore_index=0)(outs, labels).item()
        prp = math.exp(loss)
        perp = perp+prp
        count += 1
    count = count*(len(batch))
    return (perp/count)

#Training our model
# os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
lm = LM_LSTM(len(train_dataset.vocab), embedding_matrix=train_dataset.embeddings).to(device)

optim = torch.optim.SGD(lm.parameters(), lr=0.1)

train_data = DataLoader(train_dataset, batch_size=64, shuffle=True)
valid_data = DataLoader(valid_dataset, batch_size=64, shuffle=True)



for i in range(8):
    for batch in train_data:
        optim.zero_grad()
        lstm_input = batch[0]
        lstm_input = lstm_input.to(device)
        outs = lm.forward(lstm_input)
        x = outs.shape[-1]
        outs = outs.view(-1, x)
        labels = batch[1]
        labels = labels.to(device)
        labels = labels.view(-1)
        loss = nn.CrossEntropyLoss(ignore_index=0)(outs, labels)
        loss.backward()
        optim.step()

    val_perp = perplexity_func(lm, valid_data)
    val_loss = loss_func(lm, valid_data)
    print(f"Epoch {i+1} | Val Loss: {val_loss} | Val Perplexity: {val_perp}")

torch.save(lm, 'lstm.pth')

#Testing our model


model = torch.load('lstm.pth')
model = model.to(device)


#generate perplexity file

with open('2020115016-LM2-test-perplexity.txt', 'w') as f:
    for line in test_list:

            indices = list()
            lstm_input = list()
            labels = list()
            for word in word_tokenize(line):
                if word.lower() in train_dataset.vocab:
                    indices.append(train_dataset.word2idx[word.lower()])
                else:
                    indices.append(train_dataset.word2idx['<unk>'])


            a = indices
            b = indices[1:]
            pad_len = train_dataset.max_len - len(indices)
            a += [0]*pad_len
            b += [0]*(pad_len+1)

            lstm_input.append(torch.tensor(a))
            labels.append(torch.tensor(b))

            if(len(lstm_input)==0):
                continue
            lstm_input = torch.stack(lstm_input)
            lstm_input = lstm_input.to(device)
            outs = model(lstm_input)
            x = outs.shape[-1]
            outs = outs.view(-1, x)
            if(len(labels)==0):
                continue
            labels = torch.stack(labels)
            labels = labels.to(device)
            labels = labels.view(-1)
            loss = nn.CrossEntropyLoss()(outs, labels).item()
            prp = math.exp(loss)/labels.size(0)
            sen = line[:-1] + '\t' + str(prp) + '\n'
            f.write(sen)

with open('2020115016-LM2-train-perplexity.txt', 'w') as f:
    for line in train_list:
            
            indices = list()
            lstm_input = list()
            labels = list()
            for word in word_tokenize(line):
                if word.lower() in train_dataset.vocab:
                    indices.append(train_dataset.word2idx[word.lower()])
                else:
                    indices.append(train_dataset.word2idx['<unk>'])
            
           
            a = indices
            b = indices[1:]
            pad_len = train_dataset.max_len - len(indices)
            a += [0]*pad_len
            b += [0]*(pad_len+1)
            
            lstm_input.append(torch.tensor(a))
            labels.append(torch.tensor(b))  
            
            if(len(lstm_input)==0):
                continue
            lstm_input = torch.stack(lstm_input)
            lstm_input = lstm_input.to(device)
            outs = model(lstm_input)
            x = outs.shape[-1]
            outs = outs.view(-1, x)
            if(len(labels)==0):
                continue
            labels = torch.stack(labels)
            labels = labels.to(device)
            labels = labels.view(-1)
            loss = nn.CrossEntropyLoss()(outs, labels).item()
            prp = math.exp(loss)/labels.size(0)
            sen = line[:-1] + '\t' + str(prp) + '\n'
            f.write(sen)